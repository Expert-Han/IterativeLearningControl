\chapter{Robustness}

Then the relation 
\begin{align}
\label{eq:strangeMatrix}
\begin{pmatrix}
I & 0 \\ \c{A} &  \c{B}
\end{pmatrix}^T\begin{pmatrix}
-X & 0 \\ 0 & X
\end{pmatrix} 
\begin{pmatrix}
I & 0 \\ \c{A} &  \c{B}
\end{pmatrix} + 
\begin{pmatrix}
\c{C} & \c{D} \\ 0 & I
\end{pmatrix}^T
P
\begin{pmatrix}
\c{C} & \c{D} \\ 0 & I
\end{pmatrix} \preceq 0.
\end{align}
ensures the stability, if 
\begin{align}
\label{eq:PDelta}
\begin{pmatrix}
I \\ \Delta
\end{pmatrix}^T 
P
\begin{pmatrix}
I \\ \Delta
\end{pmatrix} \succeq 0, 
\end{align}
$P$ is here some symmetric matrix from the \textit{multiplicator} matrix set. 

To see it, we multiply  \eqref{eq:strangeMatrix} with 
\begin{align}
\label{eq:H}
H = \begin{pmatrix}
I \\ \Delta ( I - \c{D} \Delta)^{-1}C
\end{pmatrix}
\end{align}
right and its transpose left. 







The relation \eqref{eq:Lyap} is fulfilled if there exists some matrix $X \succ  0$, such that 
\begin{align}
\label{eq:XAB<=0}
\begin{pmatrix}
I & 0 \\ \c{A} &  \c{B}
\end{pmatrix}^T
\begin{pmatrix}
-X & 0 \\ 0 & X
\end{pmatrix}
\begin{pmatrix}
I & 0 \\ \c{A} &  \c{B}
\end{pmatrix} \preceq 0.
\end{align}
It occurs since the  multiplication of \eqref{eq:XAB<=0} with
\begin{align}
\label{eq:H}
H = \begin{pmatrix}
I \\ \Delta ( I - \c{D} \Delta)^{-1}C
\end{pmatrix}
\end{align}
right and its transpose left results in 
\begin{align}
%\begin{split}
%&	\begin{pmatrix}
%I \\ \Delta ( I - \c{D} \Delta)^{-1}C
%\end{pmatrix}^T 
H^T
\begin{pmatrix}
I & 0 \\ \c{A} &  \c{B}
\end{pmatrix}^T
\begin{pmatrix}
-X & 0 \\ 0 & X
\end{pmatrix} 
\begin{pmatrix}
I & 0 \\ \c{A} &  \c{B}
\end{pmatrix} H 
%\\= & -X + \c{A}^T X \c{A} + \c{A}^T X \c{B} \Delta(I - \Delta\c{D})^{-1} \c{C} + \c{C}^T(I - \Delta\c{D})^{-T}\c{B}^T X \c{B} (I - \Delta\c{D})^{-1}\c{C} \\
= L(\Delta)^T X L(\Delta) - X.
\end{align}

Hence our problem, to prove the robust stability, can be reduced to a certain LMI.

However, this problem is hard to solve, since this inequality does not take the structure of the uncertainty into account. The next theorem insight an idea to determine if the system is robustly stable by solving an LMI over a much smaller matrix set -- with so-called ''full-block S-procedure''.



\begin{theo}
	\label{thm:stabViaP}
	Let the symmetric matrix $X \succ 0 $ be fixed.
	Then the system \eqref{eq:LekUnc} is stable if there exists a matrix $P = P^T$, such that 
	\begin{align}
	\label{eq:PDelta}
	\begin{pmatrix}
	I \\ \Delta
	\end{pmatrix}^T 
	P
	\begin{pmatrix}
	I \\ \Delta
	\end{pmatrix} \succeq 0, 
	\end{align}
	%	\begin{align}
	%	\mathbb{P} = \{P = \begin{pmatrix}
	%	Q & \\
	%	& -Q
	%	\end{pmatrix}| \;Q \succeq 0\},
	%	\end{align}
	and additionaly 
	\begin{align}
	\label{eq:strangeMatrix}
	\begin{pmatrix}
	I & 0 \\ \c{A} &  \c{B}
	\end{pmatrix}^T\begin{pmatrix}
	-X & 0 \\ 0 & X
	\end{pmatrix} 
	\begin{pmatrix}
	I & 0 \\ \c{A} &  \c{B}
	\end{pmatrix} + 
	\begin{pmatrix}
	\c{C} & \c{D} \\ 0 & I
	\end{pmatrix}^T
	P
	\begin{pmatrix}
	\c{C} & \c{D} \\ 0 & I
	\end{pmatrix} \preceq 0.
	\end{align}
	If the inequality \eqref{eq:strangeMatrix} is strict, the monotonic convergence to zero is guaranteed. 
\end{theo}
\begin{proof}
	Let $P \in \mathbb{P}$ be arbitrary. 
	
	The first term is positive semi-definite because of \eqref{eq:XAB<=0}.
	Hence, if we can prove that the second term multiplied with $H$ as in \eqref{eq:H} right and its transpose left is positive semi-definite, this matrix $P$ will ensure the monotonic convergence of the algorithms.
	\begin{align*}
	&H^T\begin{pmatrix}
	\c{C} & \c{D} \\ 0 & I
	\end{pmatrix}^T
	P
	\begin{pmatrix}
	\c{C} & \c{D} \\ 0 & I
	\end{pmatrix} H  = 
	(I - \Delta \c{D})^{-T} \underbrace{\begin{pmatrix}
		I \\ \Delta 
		\end{pmatrix}^T P
		\begin{pmatrix}
		I \\ \Delta 
		\end{pmatrix}}_{\succeq 0}(I - \Delta \c{D})
	\succeq 0,
	\end{align*}
	If the inequality \eqref{eq:strangeMatrix} is strict, the LMI \eqref{eq:XAB<=0} is also strict and we achieve the asymptotic stability. 
	
	%	\begin{align*}
	%	&\begin{pmatrix}
	%	I \\ \delta ( I - \c{D} \delta)^{-1}C
	%	\end{pmatrix}\begin{pmatrix}
	%	\c{C} & \c{D} \\ 0 & I
	%	\end{pmatrix}^T
	%	P
	%	\begin{pmatrix}
	%	\c{C} & \c{D} \\ 0 & I
	%	\end{pmatrix} \begin{pmatrix}
	%	I \\ \delta ( I - \c{D} \delta(\delta))^{-1}C
	%	\end{pmatrix} \\
	%	=  &\c{C}^T[
	%	Q + Q \c{D} \delta (I - \delta\c{D})^{-1} + (I - \delta \c{D})^{-T} \delta\c{D}^T Q \c{D} \delta (I - \delta\c{D})^{-1} +\\
	%	&+ (I - \delta)^{-T}  Q \delta^2(I - \c{D} \delta)^{-1} \\
	%	= & \c{C}^T[ Q + Q \c{D} \delta(I - \c{D}\delta)^{-1} + (I - \c{D}\delta)^{-T} (\delta \c{D}^T Q + \\
	%	&+\delta \c{D}^T Q \c{D} \delta (I - \delta)^{-1} - \delta Q\delta(I - \c{D}\delta)^{-1})] 
	%	\c{C} \\
	%	= & \c{C}^T [Q + Q \c{D} \delta (I - \c{D}\delta)^{-1} + \\
	%	&+ (I - \delta D)^{-T}(\delta \c{D}^T Q + \delta \c{D}^T Q \c{D} \Delta(I - \c{D}\Delta)^{-1} -  Q \delta^2(I - \delta\c{D}^{-1}))
	%	]C \\
	%	= &\c{C}^T[
	%	(I -\c{D} \delta)^{-T}(I -\c{D} \delta) Q (I -\c{D} \delta) (I -\c{D} \delta)^{-1} + (I -\c{D} \delta)^{-T}(Q \c{D} \delta - \delta\c{D}^TQ \c{D} \delta +\\
	%	&+ \delta\c{D} ^T Q + \delta \c{D}^T Q \c{D} \delta - \delta Q - \delta \c{D}^T Q \c{D} \delta)(I -\c{D} \delta)^{-1}
	%	]C \\
	%	=& \c{C}^T (I -\c{D} \delta)^{T} [
	%	Q - \delta \c{D}^T Q - Q \c{D} \delta + \delta \c{D}^T Q \c{D} \delta + Q \c{D} \delta + \delta \c{D}^T Q -  Q \delta^2 - \delta I \c{D}^T Q \c{D} \delta
	%	]C \\
	%	=& \c{C}(I -\c{D} \delta)^{-T} [Q -  Q \delta^2](I -\c{D} \delta)^{-1}.
	%	\end{align*}
\end{proof}



\chapter{Unit Memory Algorithm}

Mit im \autoref{ChGrDef} eingefügten Bezeichnungen, betrachten wir lineares Input/Output Modell

\begin{align}
y = Gu + d.
\end{align}

Bezeichnet man mit $r \in \Ysp$ den Referenzsignal, so kann das Problem wie folgt formuliert werden: 

Finde input $u_\infty \in \Usp$, sodass 
	\begin{align}
	r= y_\infty  \\
	\text{mit } y_\infty = G u_\infty + d. \nonumber
	\end{align}
	
Da es in der Praktik meistens nicht möglich ist, eine perfekte Lösung zu finden, stellen wir die folgende Forderung auf unsere gewünschte Lösung auf: 
\begin{align}
\forall \varepsilon > 0 : \, ||r - y_\infty|| < \varepsilon. \label{conv_cr}
\end{align}

Wir wollen nun ein Algorithmus entwerfen, mit dem wir das $u_\infty$ iterativ annähren können. 
Wir bezeichnen mit $u_0 \in \Usp$ das Anfangsinputsignal und nehmen an, dass $d \in \Ysp$ für jede Iteration gleich ist. 

Sei $k = 0, 1, 2, \dots $ der Iterationsindex für die Anzahl der vergangenen Iterationen. 
Wir bezeichnen mit $u_k$ das Inputsignal bzw. mit $y_k$ das Outputsignal beim Schritt $k$, wobei die Iteration erfogt nach folgendem Gesetz:
\begin{align}
y_k = G u_k + d, \, k\geq 0.
\end{align}

Dann können wir für jeden Schritt den Fehler 
\begin{align}
e_k := r - y_k, \, k\geq 0
\end{align}
definieren. 
Da wir unser Fehler verkleinen wollen, soll jeder neues $u_k$ sowohl vom Output $y_k$ als auch von den Fehler $e_k$, $e_{k+1}$ abhängen. So formulieren wir \textbf{Unit Memory Algorithm}: 
\begin{align}
u = f(e_k, e_{k+1}, u_k),
\end{align}


wobei $f: \Ysp \times \Ysp \times \Usp \to \Usp$ ist eine von $k$ unabhängige Funktion. 

Ein besonderer Fall, der Wert zu betrachten ist, ist der Fall in dem wir $f$ gleich einer linearen Funktion setzen. Dann bekommt man folgendes Iterationsgesetz: 
\begin{align}
u_{k+1} = u_k + K_0 e_k + K_1 e_{k+1} \label{linear_model} \\ 
y_k = G u_k + d. \nonumber
\end{align}

Hier bezeichnen $K_0$ und $K_1$ Matrizen passenden Dimensionen. Die Wahl von den Matrizen ist der entscheidende Punkt, denn diese direkte Auswirkung auf die Konvergenz und Performance des Systems haben. 

In folgenden Unterkapiteln untersuchen wir die Performance- und Konvergenzeigenschaften, sowie die Robustheit des erstellten Models. Wir treffen eine wichtige Annahme, nähmlich dass unser Fehler monoton abnimmt: 
\begin{align}
||e_{k+1} || < ||e_k|| \text{ for all } k\geq 0.
\end{align}

Diese Annahme wollen wir im nächsten Kapitel diskutieren. 

\section{Monotone Konvergenz}

\section{Konvergenzkriterien} \label{UNA_conv_prop}

\subsection{Konvergenzkriterien für State Space Model}

Wir betrachten also den Iterationsgesetz \ref{linear_model} und wollen Kriterien an die Matrizen $K_0$ und $K_1$ stellen, sodass gewünschte Konvergenz des Fehlers gewährleistet wird. 

Aus $Gu = y-d = r - e - d$ bekommen wir
\begin{align}
e_{k+1} = e_k - GK_0 e_k - GK_1 e_{k+1},
\end{align}
und, unter der Annahme, dass $(I + GK_1)$ nicht singular ist, ergibt sich Iterationsgesetz für den Fehlerterm:
\begin{align}
e_{k+1} = L e_k, \\
\text{mit } L = (I+GK_1)^{-1}(I-GK_0)  \nonumber
\end{align}
und neuer "update"-Gesetz für Inputsignal:
\begin{align}
u_{k+1} = u_k + (K_0 + K_1 L ) e_k.
\end{align}
Die weitere Beobachtung ist, dass die Evolution des Fehlers ergibt sich zu 
\begin{align}
e_k = L^k e_0 \text{ for all } k \geq 0.
\end{align}
Hieraus kann man folgen, das das notwendige und ausrechende Kriterium für die Konvergenz des Fehlers gegen $0$ ist $\rho (L) < 1$, wobei $\rho(L)$ das Spektralradius von $L$ bezeichnet. 

Außerdem, aus der Gleichung folgt, dass die Konvergenz bzw. die Schnelle der Konvergenz hängt von den Matrizen $G, K_0$ und $K_1$ ab, sowie vom Anfangsfehler $e_0$. Es ist somit gar nicht einfach, passende Matrizen zu finden. Folgendes Theorem ermöglich dennoch in manchen Fällen sofort die Antwort auf die Frage zu geben, ob dies überhaupt möglich ist. 

\begin{theorem}{Sprektralradius vom State Space System}
Für ein $m-$input $m-$output diskretes System $S(A, B, C, D)$ und dazugehöriges Operator $L$, definiert wie oben, auf einem endlichen Zeitintervall gilt: Spektrum von $L$ ist genau gleich der Menge der Eigenwerte von $D$. Mit anderen Worten:
\begin{align}
\rho(L) = \rho(D).
\end{align}
\end{theorem}
\begin{proof}
	TODO Proof
\end{proof}

Es ist klar, dass die Monotonieeigenschaft der Folge $\{e_k\}_{k\geq 0}$ hängt von der gewälten Norm aus. Dennoch für endlichen Vektorräumen lässt sich dieses Problem umgehen: es reicht aus, nur eine Norm $|| \cdot ||$ in $\Ysp$ zu finden, in der monotone Konvergenz gewährleistet wird. Dann konvergiert die Folge auch in jeder anderen Norm. Dazu gibt es folgender Satz:%TODO Ich habe eigentlich keine Ahnung ob ich das richtig formuliert habe. Soll definetiv mir das Ergebnis nochmals anschauen
\begin{theorem}
	
	THEOREM 5.6
\end{theorem}

%TODO Theorem 5.7? 

\subsection{Relaxation Methods}
TODO: Relaxation methods $-(-\_-)-$

\section{Robustheit}
Man möchte die Konvergenz auch dann gewährleisten, wenn das wirkliche Systemsverhalten sich von vom erstellten Modell vorgesagtem Verhalten abweicht. In dargestellten Modell kann man zwei Unsicherheiten einfach sehen, nähmlich 
\begin{enumerate}
	\item Unsicherheit in dem Wert des Parameters im Plant Modell und 
	\item Unsicherheit im Struktur vom Plant. 
\end{enumerate}

Parameterunsicherheit kann man ausdrücken dadurch, dass man für ein ungenaues Parameter $p$ Intervallgrenzen angibt und betrachtet $p \in [p_l, p_u]$, wobei $p_l$ und $p_u$ sind bekannte untere und obere Intervallgrenzen. Die Strukturunsicherheit kann zwar viele Formen bei den praktischen Anwendungen annehmen, aber zum Zielen der theoretischen Analyse wird oft als additiv oder multiplikativ angenommen. Mit anderen Worten, falls $G$ stellt Plant Model dar, dann die Unsicherheitsmodelle können als
\begin{align}
G + \Delta G \text{ (additive Unsicherheitsmodel)}, \\
UG \text{ (linke multiplikative Unsicherheitsmodel)},\\
GU \text{ (rechte multiplikative Unsicherheitsmodel) }, \\ 
G^{-1} + V \text{ (inverse additive Unsicherheitsmodel)},
\end{align}
mit $\Delta G \in \mathbb{R}^{l \times m}$, $U \in \mathbb{R}^{l\times l}$ und $V \in \mathbb{R}^{m \times l}$. 

Um formale Aussagen zu treffen, führt man folgende Definition ein: 

\begin{defi}
	Für ein lineares Model $y = Gu + d$ und dazugehöriger Algorythmus für Input Update sei monotone Konvergenz des Fehlers gegen Null erfüllt: 
	\begin{align}
	||e_{k+1} || < ||e_k|| \text{ und } \lim_{k \to \infty} e_k = 0. \label{mon_prop}
	\end{align}
	Wenn die Unsicherheit ist durch den Modelfehler $U$ dargestellt wird, das resultierende ILC System heißt monoton konvergent mit Robustheit im Bezug auf Modelfehler $U$, falls die Eigenschaft der Monotone Konvergenz \ref{mon_prop} bleibt erhalten. 
\end{defi}

Wie es im Unterkapietel \ref{UNA_conv_prop} besprochen wurde, reicht $\rho(L) < 1$ für die Konvergenz des Fehlers gegen Null. Setzt man in $L = (I + GK_1)^{-1}(I - GK_0)$ $K_1 = 0$, so muss $\rho(I - GK_0) < 1$ gelten. Dennnoch bei der Anwendung des Modells auf das reeles Plant $G + \Delta G$, die Matrix $L$ wird zu $I - (G+ \Delta G)K_0 = L + \Delta L$ mit $\Delta L = - \Delta G K_0$. Nun Konvergenz ist dann gewährleistet, wenn $|| L + \Delta L|| < 1$. $||L + \Delta L || \leq ||L || + ||\Delta L||$ schließt den Beweis des folgenden Satzes ab: 
%TODO $u_{k+1} = u_k + K_0 e_k$ reinschreiben, ansonsten ist beim Satz nicht klar wo es herkommt

\begin{theorem}
	
	Mit der Notation von oben, ein ausreichendes Kriterium für die monotone Konvergenz mit Robustheit für den ALgorithmus $u_{k+1} = u_k + K_0 e_k$ in der Sicht der additiven Unsicherheit ist 
	\begin{align}
	||\Delta L || = ||\Delta G K_0||< 1 - ||L||.
	\end{align}
\end{theorem} 

Mit dieser einfachen Normenungleichung kann man sofort den Grad der Robustheit des Systems sehen, die immer dann gewährleistet werden kann, wenn $|| L || < 1$. 

\subsection{Robustheit in der Sicht der Zustandsunsicherheit}

Es sei nun das Model exakt, aber der Anfangswert bei jeder Iteration kann abweichen. Da im betrachtenden Modell $y = G u + d$ nur der Term $d$ von dem Anfangswert $x_0$ abhängt, kann man diese Unsicherheit durch das Ersetzen von $d$ durch einen von dem Iterationsschritt k abhängigen Term $d_k$. Aus $u_{k+1} = u_k + K_0 e_k + K_1 E_{k+1}$ und dem Modell $y_k = G u_k + d_k$ neuer Verlauf des Fehlers wird zu 
\begin{align}
e_{k+1} = L e_k + (I + G K_1)^{-1} (d_k + d_{k+1}), \, k\geq 0.
\end{align}