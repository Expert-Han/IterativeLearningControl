\chapter{Unit Memory Algorithm}
 
 We consider the mathematical model of the form given in \eqref{eq: Gu + d}. 
 
 For the reference signal $r$ let $y_\infty = r$ be perfect tracking requirement, with input-output relation $y_\infty = G u_\infty + d$. 
 
 The aim is to improve the actual tracking -- that means, to reduce the norm of the error $e$ until desired accuracy $\varepsilon$: 
 \begin{align}
 \label{eq: convReq}
 \begin{split}
  &\text{for any } \varepsilon >0 \text{ find some } y \in \R^{m(N+1)} \text{ such that } ||e || = ||r - y|| < \varepsilon \\
 &\text{for some norm } ||\cdot|| \text{ in } \R^{m(N+1)}.
 \end{split}
  \end{align}
 
Not every algorithm can ensure the convergence of the error to 0. In this case, we try just to reduce the error to some $e_\infty \in \R^{m(N+1)}$, such that $||e_\infty|| < ||e||$. 

\begin{alg}
		A \textbf{Unit Memory Algorithm} is given by the update low 
		\begin{align}
		\label{eq: inpUpdR}
		u_{k+1} &= u_k + K_0 e_k,& &\text{ (The Input Update Rule)} \\
		\label{eq: inpUpdR:PlantDynamics}
		\text{with } y_k &= G u_k + d, \; k\geq 0& &\text{ (Plant Dynamics)}. 
		\end{align}
		The initial choice of input signal to be given as $u_0 \in \R^{m(N+1)}$ (e.g. the signal we got from controller). 
		
	    The error evolution is given by the equation 
	    \begin{align}
	    \label{eq: errEvol}
	    e_{k+1} = (I-G K_0) e_k = (I - G K_0)^k e_0 = : L^k e_0, \; k\geq 0,
	    \end{align}
	    with initial error $e_0 = r - G u_0 - d$. 
\end{alg}

With formula \eqref{eq: errEvol} the convergence requirement \eqref{eq: convReq} can be achieved by ensuring $||L||<1$. Indeed, it is enough to calculate the maximal absolute value of the matrix $L$. This illustrates the following theorem. 
%\begin{theo}
%	\label{thm: rho(L)<1}
%	Let the sequence $\{e_k\}_{k\geq 0}\subset \R^{m (N+1)}$ be generated by the iteration $e_{k+1} = L e_k$, $k \geq 0$, for some matrix $L \in \R^{m(N+1)\times m(N+1)}$. 
%	Then the sequence converges to zero if and only if 
%	\begin{align*}
%	\rho(L) < 1.
%	\end{align*}
%\end{theo}

For the $m$-input $m$-output systems convergency test can be reduced to the estimation of the maximal absolute value of the matrix $D$: 

\begin{theo}
    For an $m-$input $m-$output system $(A,B,C,D)$ let $L$ be given as in \eqref{eq: errEvol}. Then  the spectrum of $L$ is precisely the set of the eigenvalues of $D$ and hence $\rho(L) = \rho(D)$. Especially, the sequence from theorem \ref{thm: rho(L)<1} converges to 0 if and only if $\rho(D) < 1$. 
\end{theo}

\begin{theo}
	Norm equivalency? If in the recall chapter: with Q and R. 
\end{theo}

\section{Inverse Model Algorithms}
For the given relation $y = Gu + d$ the perfect tracking is achieved if there exists some $u_\infty$ with $r = G u_\infty + d$. If $G$ is invertable, $u_\infty$ can be calculated as $u_\infty = G^{-1}(r - d)$. 

This motivates to choose $K_0$ in \eqref{eq: inpUpdR} as $K_0 = G^{-1}$. 

The idea of the inverse model can be also used if the matrix $G$ is singular, since it can still have right or left inverse. 

\subsection{Right Inverse Model Algorithm}
\begin{alg}
	\label{alg: rightInv}
	Let the matrix $G$ have a right inverse $G_R$: 
	\begin{align*}
	G G_R = I. 
	\end{align*}
	Set $K_0 = \beta G_R$, where $\beta$ is a real scalar ''learning gain''. Then the input update rule is given by 
	\begin{align}
	u_{k+1} = u_k + G_R e_k, \; k\geq 0,
	\end{align} 
	and the error evolution satisfies 
	\begin{align}
	\label{eq: errRightInv}
	e_{k+1} = (1- \beta) e_k = (1-\beta)^k e_0, \; k\geq 0. 
	\end{align}
	In particular, the error $e_k$ converges to zero for any initial error $e_0$ if and only if 
	\begin{align*}
		0 < \beta < 2.
	\end{align*}
\end{alg}

The existence of a right inverse is guaranteed by theorem \ref{thm: relGandK} if and only if $\im(D) = m$ -- with other words, the matrix $D$ must have full row rank. 

The convergence rate of the algorithm \ref{alg: rightInv} depends upon the chosen gain $\beta$. For $\beta = 1$ we get convergence in one iteration -- since fast, this choice is not robust at all. Even little perturbations in the model or numerical inaccuracy can lead to false result. -> Ref chapter ????. 

\subsection{Left Inverse Model Algorithm}
If the matrix $G$ still have a left inverse, one can also improve the tracking -- but this time it is not always possible to achieve convergence to zero. 

\begin{alg}
	\label{alg: leftInv}
	Let the matrix $G$ have a left inverse $G_L$: 
	\begin{align*}
	G_L G = I. 
	\end{align*}
	Set $K_0 = \beta G_L$, where $\beta$ is a real scalar ''learning gain''. Then the input update rule is given by 
	\begin{align}
	u_{k+1} = u_k + G_L e_k, \; k\geq 0,
	\end{align} 
	and the error evolution satisfies 
	\begin{align}
	\label{eq: errLeftInv}
	e_{k+1} = (I- \beta G G_L) e_k = (I+  \left[(1-\beta)^k - 1\right] G G_L) e_0, \; k\geq 0.
	\end{align}
	Monotonic convergence to 
	\begin{align}
	\label{eq: leftInvErrLim} 
	e_\infty  = \lim_{k\to\infty} e_k = (I - G G_L)e_0
	\end{align} is guaranteed if and only if 
	\begin{align*}
	0 <\beta < 2.
	\end{align*}
\end{alg} 

\begin{proof}
	Using the relation $(GG_L)^2 = GG_LGG_L = GG_L$ the error evolution formula \eqref{eq: errLeftInv} be proven by using mathematical induction: 
        
        for $k = 0$ it follows: 
        \begin{align*}
        e_1 = (I - \beta G G_L)e_0 = (I + GG_L - GG_L - \beta G G_L)e_0 = (I + \left[(1 - \beta) + 1\right]GGL).
        \end{align*}
        
        For $k \in \N$ it follows: 
        \begin{align*}
        e_{k+1} &= ( I - \beta G G_l)e_k = ( I - \beta G G_L) (I+  \left[(1-\beta)^{k-1} - 1\right] G G_L) e_0 = \\
        & = \left(I - \beta G G_L + (1-\beta)^{k-1}GG_L - GG_L - \beta GG_L\left[(1 - \beta)^{k-1} - 1\right]GG_L\right)e_0=\\
        & = \left(I - (\beta - (1-\beta)^{k-1} + \beta\left[(1 - \beta)^{k-1} - 1\right])G G_L\right)e_0\\
        & = \left(I - \left(\beta - (1-\beta)^{k-1} + 1 + \beta(1-\beta)^{k-1} - \beta\right)GG_L\right)e_0\\
        & = (I+  \left[(1-\beta)^k - 1\right] G G_L) e_0
        \end{align*}
        
        To prove \eqref{eq: leftInvErrLim} recall that 
        \begin{align*}
        \R^{m(N+1)} = \im(G)\oplus \ker (G_L). 
        \end{align*}
        Since $e_0$ can be written as 
        \begin{align*}
        e_0 = G w_0 + v_0,
        \end{align*}
        where $v_0 = (I - G G_L)e_0 \in \ker (G_L)$  is uniquely defined and $w_0 = G_L e_0 \in \R^{l(N+1)}$, it follows 
        \begin{align*}
        e_{k+1} = (1+ \left[(1 - \beta)^k - 1\right]GG_L)e_0 = (1-\beta)^{k} GG_Le_0 + v_0.
        \end{align*}
        Then the error $e_k$ converges to $v_0$ for $k \to \infty$. 
\end{proof}

As it has been seen above, the monotonic convergence to zero will only occur if $v_0 = 0$ -- this befalls if $e_0 \in \im(G)$. As $e_0 = r - Gu_0 - d$, the last condition becomes $r - d \in \im(G)$. This simply means that there exist a control input $u \in \R^{l(N+1)}$, such that the desired value $r$ is tracked exactly. 
Clearly, if there does not exist such an input, the algorithm just converges to the smallest possible error. 

\subsection{Choice of  learning gain $\beta$}
The benefit of the algorithms \ref{alg: rightInv} and \ref{alg: leftInv} is intuitive understanding and  need to adapt only one control parameter $\beta$. 

In simply words, for $\beta$ close to 0 or 2, we get slow convergence
with good robustness properties. For $\beta$ close to unity we have rapidly convergence,
but with reduced degree of robustness. For $\beta = 1$ we get convergence in one iteration.



\begin{align}
A^T P A - P + Q = 0
\end{align}













 