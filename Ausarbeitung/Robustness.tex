\chapter{Robustness of the ILC Algorithms}
If our model is not precisely known a chosen ILC algorithm might not provide the desired result. 
Therefore it is useful to consider how robust the algorithms are. 
In this thesis, we are going to consider the parametric uncertainty. That means, we do not know the true value of a real parameter (or the parameters) but know some range in which this parameter must lie. 

We can normalize such uncertainties without loss of generality, as any uncertainty $\delta \in [a,b] \subset \R$ can be expressed as 
\begin{align}
\label{eq:rob:del=del0+...}
\delta = \delta_0 + w \hat{\delta}
\end{align}
with $\hat{\delta} \in [-1,1]$, nominal value $\delta_0 \in \R$ and weight $w\in \R$. 

%Such uncertainties can be described as a set of the real diagonal matrices \cite{SchRC}. Each  matrix $\hat{\Delta}$ from this set can be  bounded
% with a matrix norm $|| \cdot ||$. With this norm, we can measure ''how much'' robust is our algorithm or system. Typically, one can bound this norm with 1, and put the scalings into the weights. {\color{red} Hier kommt vielleicht ein Beispiel, wie man das mit dem Rausziehen und den Weights macht}.
%

In order to guarantee the convergence of the algorithms despite such parametric uncertainties it is required that \eqref{eq:e_k}  stays stable for all possible uncertainties. 

Let us consider, how the uncertainty looks in the lifted system like and deliberate over the method to solve the robustness issue. 


%\begin{align}
%\hat{\Delf} = \{\hat{\Delta} = \begin{pmatrix}
%\delta_1 & & & \\
%&  \delta_2 & & \\
%& &  \ddots &  \\
%& & & \delta_\tau 
%\end{pmatrix}| \;\delta_i \in [-1,1] \text{ for } i = 1, 2, \dots, \tau\}
%\end{align}
%with $\tau$ of the fit dimension.



%For example, let us look at the system (A, B, C, D), with 
%\begin{align}
%A = \begin{pmatrix}
%-.5 + a & \\
%0       & 0 -.4 - a
%\end{pmatrix}, 
%B = \begin{pmatrix}
%1 \\ 1
%\end{pmatrix}, C = \begin{pmatrix}
%.1 & .2
%\end{pmatrix} \text{ and } 
%D = 1.
%\end{align}
%
%The uncertain parameter $a$ has the nominal value $\hat{a} = 1.5$ and variability $a \in [1.3, \; 1.8].





{\color{red}$<$Einfuehrung$>$}


\section{Uncertain Lifted System} 

Let us consider a system \eqref{eq:GP} with uncertainty $\hat{\Delta} $ from the set 
\begin{align}
\label{eq:Delta_hat_set}
\hat{\Delf} =
\left\{\begin{array}{c|c}
\begin{pmatrix}
\delta_1 I_{p_1} & & &\\
& & \ddots &\\
& & &\delta_\tau I_{p_\tau}
\end{pmatrix} & \; \delta_l \in [-1,\;1], p_l \in \N \text{ for } l = 1, 2, \dots , \tau .
\end{array} \right\}.
\end{align}
$\tau \in \N$ is the number of the uncertain parameter.

Each uncertain system $\left[\begin{array}{c|c}
A & B \\\hline C &D
\end{array}\right]\left(\hat{\Delta}\right)$
we can rewrite as a feedback interconnection of the system 
$
\left[\begin{array}{c|c c}
A & B_1 & B_2 \\\hline
C_1 & D_{11} & D_{12} \\
C_2 & D_{21} & D_{22}
\end{array}\right]
$
and $\hat{\Delta}$. 
Then the original system can be find as star product (or linear fractional transformation, LFT)
\begin{align}
\left[\begin{array}{c|c}
A & B \\\hline C &D
\end{array}\right] = \hat{\Delta} \star \left[\begin{array}{c|c c}
A & B_1 & B_2 \\\hline
C_1 & D_{11} & D_{12} \\
C_2 & D_{21} & D_{22}	
\end{array}\right]
\end{align}

We illustrate it in the Figure \ref{fig:Rob:LFR}. 
We speak here about the ''pulling out'' the uncertainty, or the \textit{linear fractional representation} (LFR). 


With LFR we rewrite the system \eqref{eq:GP} as 
\begin{align}
\label{eq:uncABCD}
\begin{split}
x(t+1) &= A x(t) + B_1 u(t) + B_2 w(t), \\
y(t)   &= C_1x(t) + D_{11} u(t) + D_{12} w(t), \\
z(t)   &= C_2x(t) + D_{21} u(t) + D_{22}w(t),\\
w(t) &= \hat{\Delta} z(t),\\
x(0)& \in \R^{n}, t = 0,1,2, \dots, N, 
\end{split}
\end{align}
with error $e(\cdot) = r(\cdot) - y(\cdot)$. 


\begin{figure}
	\begin{center}
		\begin{tikzpicture}[>=stealth]
		%coordinates
		\coordinate (orig)   at (0,0);
		
		\coordinate (AroneA) at (-3,1.5);
		\coordinate (ArtwoA) at (5,.5);
		\coordinate (ArthrA) at (-1.5,.5);
		\coordinate (LLA)    at (0,0);
		\coordinate (LLD)    at (1.2,4);
		
		%nodes
		\node[draw, minimum width=2cm, minimum height=2cm, anchor=south west, align=center, fill = gray!20] (A) at (LLA) {$\left[\begin{array}{c|c c}
			A & B_1 & B_2 \\\hline C_1 & D_{11} & D_{12} \\ C_2 & D_{21} & D_{22}
			\end{array}\right]$};
		\node[draw, minimum width=1.2cm, minimum height=1cm, anchor=south west, text width=1cm, align=center ,fill = yellow!20] (D) at (LLD) {$\hat{\Delta}$};
		
		\draw[->] (ArthrA) -- node[above]{$u_k$} ($(A.180) + (0,-1/2)$);
		\draw[->]  ($(A.east) - (0,.5)$) -- node[above]{$y_{k}$}(ArtwoA);
		
		\draw[->] ($(A.0) + (0,1/2)$) -| ++ (1,1.5) -- node[right, pos=0.2]{$z_k$}++ (0,1.5) -- ($(D.east)$);		
		\draw[->] ($(D.west)$) -| ++ (-2.3,-1.5) -- node[left, pos=0.2]{$w_k$} ++ (0,-1.5) -- ($(A.180) + (0,1/2)$);
		\end{tikzpicture}
		
	\end{center}
	\caption{Uncertain system as feedback interconnection of LFR and the uncertainty $\hat{\Delta}$} \label{fig:Rob:LFR}
\end{figure}
The corresponding lifted  system has the form 
\begin{align}
y &= G_1 u + G_2 w + d_1, \\
z &= G_3 u + G_4 w + d_2, \\
e &= r - y,\\
w &= \Delta z,
\end{align}
with the matrices
\begin{align}
G_1 &= \nonumber \\
&=  \begin{pmatrix}
	D_{11} & 0 & \cdots & 0 & 0 & 0 \\
	C_1B_1 & D_{11} & \cdots & 0 & 0 & 0\\
	C_1AB_1 & C_1B_1 & \cdots & 0 & 0 & 0\\
	\vdots & \vdots & \ddots & \vdots  & \vdots & \vdots \\
	C_1A^{N-2} B_1 & C_1A^{N-3} B_1 &\dots &C_1B_1 & D_{11}& 0\\
	C_1A^{N-1} B_1 & C_1A^{N-2} B_1 &\dots &C_1AB_1 & C_1B_1& D_{11}\\
\end{pmatrix},
\end{align}
\begin{align}G_2 & = \nonumber\\
&= \begin{pmatrix}
D_{12} & 0 & \cdots & 0 & 0 & 0 \\
C_1B_2 & D_{12} & \cdots & 0 & 0 & 0\\
C_1AB_2 & C_1B_2 & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \ddots & \vdots  & \vdots & \vdots \\
C_1A^{N-2} B_2 & C_1A^{N-3} B_2 &\dots &C_1B_2 & D_{12}& 0\\
C_1A^{N-1} B_2 & C_1A^{N-2} B_2 &\dots &C_1AB_2 & C_1B_2& D_{12}\\
\end{pmatrix},
\end{align}
\begin{align}
G_3 & = \nonumber\\
&= \begin{pmatrix}
D_{21} & 0 & \cdots & 0 & 0 & 0 \\
C_2B_1 & D_{21} & \cdots & 0 & 0 & 0\\
C_2AB_1 & C_2B_1 & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \ddots & \vdots  & \vdots & \vdots \\
C_2A^{N-2} B_1 & C_2A^{N-3} B_1 &\dots &C_2B_1 & D_{21}& 0\\
C_2A^{N-1} B_1 & C_2A^{N-2} B_1 &\dots &C_2AB_1 & C_2B_1& D_{21}\\
\end{pmatrix},
\end{align}
\begin{align}
G_4 & = \nonumber\\
&= \begin{pmatrix}
D_{22} & 0 & \cdots & 0 & 0 & 0 \\
C_2B_2 & D_{22} & \cdots & 0 & 0 & 0\\
C_2AB_2 & C_2B_2 & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \ddots & \vdots  & \vdots & \vdots \\
C_2A^{N-2} B_2 & C_2A^{N-3} B_2 &\dots &C_2B_2 & D_{22}& 0\\
C_2A^{N-1} B_2 & C_2A^{N-2} B_2 &\dots &C_2AB_2 & C_2B_2& D_{22}\\
\end{pmatrix},
\end{align}
and vectors 
\begin{align}
d_1 = \begin{pmatrix}
C_1 x_0 \\ C_1A x_0 \\ C_1A^2 x_0 \\ \vdots \\ C_1A^Nx_0
\end{pmatrix},
\end{align}
\begin{align}
d_2 = \begin{pmatrix}
C_2 x_0 \\ C_2A x_0 \\ C_2A^2 x_0 \\ \vdots \\ C_2A^Nx_0
\end{pmatrix}.
\end{align}
$\Delta$ is here a block diagonal matrix 
\begin{align}
\Delta = \diag(\underbrace{\hat{\Delta}, \hat{\Delta}, \dots, \hat{\Delta}}_{(N+1) \text{ times}})
\end{align}
and we define 
\begin{align}
\Delf = \{\diag(\hat{\Delta}, \hat{\Delta}, \dots, \hat{\Delta})| \; \hat{\Delta}\in \hat{\Delf}\}.
\end{align}




Using LFT, we get 
\begin{align}
\label{eq:rob:y = G(Delta) + d(Delta)}
y&= G(\Delta)u + d(\Delta) := \left[G_1 + G_2 \Delta (I - G_4\Delta)^{-1}G_3\right] u + \left[G_2 \Delta (I - G_4\Delta)^{-1} d_2 + d_1\right], 
\end{align}
while the inverse of the matrix $(I - G_4\Delta)$ is assumed to exist. It is always the case if the norm of the matrix $G_4$ is (strictly) bounded by 1 \cite{SchRC}.



%TODOO: A -> A_1

With ILC algorithms we get the input update rule
\begin{align}
u_{k+1} = u_k + Ke_k,
\end{align}
and hence 
\begin{align}
\label{eq:LekUnc}
e_{k+1} = L(\Delta)e_k = (I - G(\Delta)K) e_k.
\end{align}
If we pull out the uncertainty from $L(\Delta)$, we get 
%
%From our algorithms, we get the matrix $L$, which depends on the model and hence it depends on
% the given uncertainty. 
%With the methods from \cite{SchRC} we can pull out the uncertainty, and get the controlled system 
\begin{align}
\begin{split}
\left(\begin{array}{c}
e_{k+1} \\ \hline z_k
\end{array}\right) &= 
\left(\begin{array}{c| c}
\c{A} & \c{B} \\
\hline
\c{C} & \c{D}
\end{array}\right) \left(\begin{array}{c}
e_{k}  \\\hline w_k
\end{array}\right):= 
\left(\begin{array}{c|c}
I - G_1 & -G_2 \\
\hline
G_3 K & G_4
\end{array}\right)
\left(\begin{array}{c}
e_{k}  \\\hline w_k
\end{array}\right),\\
w_k &= \Delta z_k,
\end{split}
\end{align}
with the uncertain system defined through the uncertain matrix 
\begin{align}
\label{eq: LstarDelta}
L(\Delta) = \Delta \star \left(\begin{array}{c| c}
\c{A} & \c{B} \\
\hline
\c{C} & \c{D}
\end{array}\right) = \c{A} + \c{B} \Delta (I - \c{D}\Delta)^{-1}\c{C}. 
\end{align}
 We are interested in ensuring of the stability for the system \eqref{eq:LekUnc}. 
 
 The most obviously way is to ensure $||L(\Delta)||<1$ for all uncertainties $\Delta \in \Delf$.
 %, since this implies 
% \begin{align}
% ||e_{k+1}|| = 	||L(\Delta)^ke_0|| \leq ||L(\Delta)||^k ||e_0|| \to 0 \text{ for } k \to \infty||.
% \end{align}
However, this criteria might be too restrictive. We illustrate it on the following example. 

\begin{exam}
	\label{exp:rob:bad_example}
Let us consider the system 
\begin{align}
e_{k+1}  = L(\alpha) e_k = \begin{pmatrix}
.1 & \alpha \\ 0 & .1
\end{pmatrix}e_k
\end{align}
with uncertain parameter $\alpha \in \R$.
Then the norm $||L(\alpha)||$ is proportional to $|\alpha|$.

For example, with the spectral norm, we can ensure stability only for $|\alpha|<1$. 


On the other hand, one can directly see, that the system is stable for all $\alpha \in \R$, since the eigenvalues of the matrix equal $.1$. 	
\end{exam}

This example motivates to devise another method for ensuring the stability of the uncertain systems. 
Much more elegant results we can reach using Full-Block S-Procedure. 

 

\section{Full-Block S-Procedure}

First, let us consider the uncertainty $\Delta$ without taking into account its structure. 
Using the Lyapunov LMI we formulate the following Theorem. 

\begin{theo}
	\label{thm:Lyap}
	Suppose that there exists some positive definite matrix $X$ satisfying 
	\begin{align}
	\label{eq:Lyap}
	\begin{pmatrix}
	I \\ L(\Delta) 
	\end{pmatrix}^T
	\begin{pmatrix}
	-X & 0 \\ 0 & X
	\end{pmatrix}
	\begin{pmatrix}
	I \\ L(\Delta) 
	\end{pmatrix} = L(\Delta)X L(\Delta) - X \preceq 0 \text{ for all } \Delta \in \Delf.
	\end{align}
	Then the discrete dynamical system \eqref{eq:LekUnc} 
	is stable for all $\Delta \in \Delf$ and for all $e_0 \in \R^{m ( N+1)}$.
	It is asymptotically stable if the inequality is strict. 
\end{theo}
\begin{proof}
	First, let us prove the Theorem for the strict LMI. Then we can find some $\gamma \in (0,1)$, such that
	\begin{align}
	\label{eq:proof0}
	\begin{pmatrix}
	I \\ L(\Delta)
	\end{pmatrix}
	\begin{pmatrix}
	-\gamma X & 0 \\ 0 & X
	\end{pmatrix}
	\begin{pmatrix}
	I \\ L(\Delta)
	\end{pmatrix} = L(\Delta)^T X L(\Delta) - \gamma X \prec 0 \text{ for all } \Delta \in \Delf.
	\end{align}	
	

	Moreover, since $X \succ 0$, there exist some $\alpha, \beta > 0$, such that 
	\begin{align}
	\label{eq:proof1}
	\alpha I \prec X \prec \beta I.
	\end{align}
	
	Set $\eta_k := e_k^T X e_k$. Then we have
	\begin{align}
	\label{eq:proof2}
	\eta_{k+1} = e_{k+1}^T X e_{k+1} = e_k^T L(\Delta)^T X L(\Delta) e_k \leq \gamma e_k X e_k = \gamma \eta_k \text{ for all } k\geq 0.
	\end{align}
    From \eqref{eq:proof1} we get
	\begin{align}
	\alpha || e_k ||^2 \leq \eta_k \leq \beta ||e_k||^2 \text{ for all } k\geq 0,
	\end{align}
	and with \eqref{eq:proof2} it follows
	\begin{align}
	\label{eq:proof3}
	||e_k||^2 \leq \frac{1}{\alpha}\eta_k \leq \frac{\gamma^k}{\alpha}\eta(0) \leq \frac{\beta}{\alpha} \gamma^k ||e_0||^2. 
	\end{align}
	
	
	Since $\gamma \in (0,\;1)$, $||e_k||^2 \to 0$ for $k \to \infty$, and taking the square root yields the claim. 
	
	For not strict case, we set in \eqref{eq:proof2} and \eqref{eq:proof3} $\gamma = 1$, and replace in \eqref{eq:proof0} ''$\prec$'' with ''$\preceq$''. Then $||e_k||^2$ (and hence $||e_k||$) is bounded over all $k\geq 0$ and hence the system \eqref{eq:LekUnc} is stable. 
	
\end{proof}

In this Theorem, we still have the uncertainty in our LMI problem. 
To get rid of it, we fix the matrix $X\succ 0$ and try to find a multiplier $P = P^T$, which fulfills 
\begin{align}
	\label{eq:PDelta}
	\begin{pmatrix}
		I \\ \Delta
	\end{pmatrix}^T 
	P
	\begin{pmatrix}
		I \\ \Delta
	\end{pmatrix} \succeq 0,
\end{align}

and, additionally,
	\begin{align}
	\label{eq:strangeMatrix}
	\begin{pmatrix}
		I & 0 \\ \c{A} &  \c{B}
	\end{pmatrix}^T\begin{pmatrix}
		-X & 0 \\ 0 & X
	\end{pmatrix} 
	\begin{pmatrix}
		I & 0 \\ \c{A} &  \c{B}
	\end{pmatrix} + 
	\begin{pmatrix}
		\c{C} & \c{D} \\ 0 & I
	\end{pmatrix}^T
	P
	\begin{pmatrix}
		\c{C} & \c{D} \\ 0 & I
	\end{pmatrix} \preceq 0.
\end{align}

If we can find such a matrix $P$, the stability of the system \eqref{eq:LekUnc} can be proven. 

We formulate it as the following Theorem. 

\begin{theo}
	\label{thm:stabViaP}
	Let $\mathbb{P}$ be a subset of the symmetric matrices, and $X \succ 0 $ be fixed.
	Then the system \eqref{eq:LekUnc} is stable if there exists a matrix $P = P^T \in \mathbb{P}$, such that \eqref{eq:PDelta} and \eqref{eq:strangeMatrix} are fulfilled. 
	If the inequality \eqref{eq:strangeMatrix} is strict, system becomes asymptotic stable. 
\end{theo}

\begin{proof}	
	Let $P = P^T \in \mathbb{P}$ be some matrix, which fulfills the relations \eqref{eq:PDelta} and \eqref{eq:strangeMatrix}.
	
	We multiply \eqref{eq:strangeMatrix} with 
	\begin{align}
		H = \begin{pmatrix}
			I \\ \Delta ( I - \c{D} \Delta)^{-1}C
		\end{pmatrix}
	\end{align}	
right and its transpose left. Then the first term results in 
\begin{align}
%\begin{split}
%&	\begin{pmatrix}
%I \\ \Delta ( I - \c{D} \Delta)^{-1}C
%\end{pmatrix}^T 
H^T
\begin{pmatrix}
I & 0 \\ \c{A} &  \c{B}
\end{pmatrix}^T
\begin{pmatrix}
-X & 0 \\ 0 & X
\end{pmatrix} 
\begin{pmatrix}
I & 0 \\ \c{A} &  \c{B}
\end{pmatrix} H 
%\\= & -X + \c{A}^T X \c{A} + \c{A}^T X \c{B} \Delta(I - \Delta\c{D})^{-1} \c{C} + \c{C}^T(I - \Delta\c{D})^{-T}\c{B}^T X \c{B} (I - \Delta\c{D})^{-1}\c{C} \\
= L(\Delta)^T X L(\Delta) - X.
\end{align}
	
	Hence, if we can prove that the second term multiplied with $H$ right and its transpose left is positive semi-definite, this matrix $P$ will ensure the monotonic convergence of the algorithms. Multiplying $H$ yields to
	\begin{align*}
	&H^T\begin{pmatrix}
	\c{C} & \c{D} \\ 0 & I
	\end{pmatrix}^T
	P
	\begin{pmatrix}
	\c{C} & \c{D} \\ 0 & I
	\end{pmatrix} H  = 
	(I - \Delta \c{D})^{-T} \underbrace{\begin{pmatrix}
		I \\ \Delta 
		\end{pmatrix}^T P
		\begin{pmatrix}
		I \\ \Delta 
		\end{pmatrix}}_{\succeq 0}(I - \Delta \c{D})
	\succeq 0,
	\end{align*}
	what proofs the first statement. 
	If the inequality \eqref{eq:strangeMatrix} is strict, the LMI \eqref{eq:Lyap} is also strict and we achieve the asymptotic stability. 
\end{proof}



Hence, the robustness issue can be reduced to a certain LMI over a multiplier set $\mathbb{P}$. 

The choosing of the set $\mathbb{P}$ is here the cornerstone. It must be large enough, to be able to find such multiplier $P$, if the system is stable, but also adequate small, such that the searching action is not too costly. 
 
Let us begin with a single uncertain parameter $\delta$. Then the uncertainty $\Delta = \delta I$ with $\delta \in [-1,\; 1]$. 
A good choice for multiplier set is 
\begin{align}
\mathbb{P}= \{ \begin{pmatrix}
S & R \\ R^T & -S
\end{pmatrix}| \; S \succeq 0 , \; R + R^T = 0\}.
\end{align}
There is 
\begin{align}
\begin{pmatrix}
I \\ \delta I
\end{pmatrix}^T 
\begin{pmatrix}
S & R \\ R^T & -S
\end{pmatrix}
\begin{pmatrix}
I \\ \delta I
\end{pmatrix} = \underbrace{(1 - \delta^2 )}_{\geq 0}S + \delta\underbrace{(R + R^T)}_{=0} \succeq 0,
\end{align}
and hence the requirements on the set $\mathbb{P}$ for the Theorem \eqref{thm:stabViaP} are fulfilled. 

For $\tau \in \N$ uncertain real parameters the uncertainty has the form 
\begin{align}\Delta= \diag(\underbrace{\hat{\Delta}_\tau, \hat{\Delta}_\tau, \dots, \hat{\Delta}_\tau}_{(N+1) \text{ times }})\end{align}
with
\begin{align}
\hat{\Delta}_\tau = \begin{pmatrix}
\delta_1 I_{p_1}& & & \\
& \delta_2 I_{p_2} & & \\
& & \ddots & \\
& & & \delta_\tau I_{p_\tau}
\end{pmatrix}.
\end{align}

If we choose a permutation matrix $\c{P}$, such that 
\begin{align}
\label{eq:rob:PDelta}
\Delta_\text{perm}= 
\c{P} \Delta =
\begin{pmatrix}
\delta_1 I_{(p_1\cdot N)} & & &\\
& \delta_2 I_{(p_2\cdot N)} & & \\
& & \ddots & \\
& & & \delta_\tau I_{(p_\tau \cdot N)}
\end{pmatrix}.
\end{align}
the convergence properties of the algorithm are not impacted: $G(\Delta)$ in \eqref{eq:rob:y = G(Delta) + d(Delta)} can be rewritten as 
\begin{align}
G(\Delta) &= G_1 + G_2  \c{P}^T \c{P} \Delta(I - G_4 \c{P}^T \c{P} \Delta)^{-1}G_3 \\
&= 
(\c{P}\Delta) \star \left[\left(\begin{array}{c | c}
G_1 & G_2 \\ \hline
G_3 & G_4
\end{array}\right)\begin{pmatrix}
I & \\ & \c{P}^T
\end{pmatrix}\right],
\end{align}
and the uncertain vector $d(\Delta)$ as 
\begin{align}
d(\Delta) &= G_2\c{P}^T\c{P} \Delta (I - G_4\c{P}^T\c{P}\Delta)^{-1} d_2 + d_1 \\
&= 
(\c{P}\Delta) \star \left[\left(\begin{array}{c | c}
d_1 & G_2  \\ \hline
d_2 & G_4 
\end{array}\right)\begin{pmatrix}
I & \\ & \c{P}^T
\end{pmatrix}\right].
\end{align}

Hence we can use the structure as in \eqref{eq:rob:PDelta} without loss of generality.
The multiplier set can be chosen as 
\begin{align}
\mathbb{P}_\tau =\left\{\begin{array}{c|c}
\begin{pmatrix}
S & R \\ R^T & -S
\end{pmatrix}
& \; \begin{array}{c} S = \diag(S_1, S_2, \dots, S_\tau)\succeq 0,\\ R = \diag(R_1, R_2, \dots, R_\tau), R + R^T = 0\end{array} \end{array}
\right\}.
\end{align}

For $P_\tau \in \mathbb{P}_\tau$
\begin{align}
\begin{pmatrix}
I \\ \Delta_\text{perm}
\end{pmatrix}^T
P_\tau
\begin{pmatrix}
I \\ \Delta_\text{perm}
\end{pmatrix}
= 
\begin{pmatrix}
(1 - \delta_1^2)S_1 & & &\\
& (1 - \delta_2^2) S_2 & & \\
& & \ddots & \\
& & & (1- \delta_\tau^2) S_\tau
\end{pmatrix}.
\end{align}



Go back to the example \ref{exp:rob:bad_example}. 
Let $\alpha \in [0,2]$ be an uncertainty with the nomimal value $\alpha_0 = 1$.
With linear fractional transformation the system 
\begin{align}
e_{k+1} = \begin{pmatrix}
.1 & \alpha \\ 0 &.1
\end{pmatrix}e_k
\end{align}
results in 
\begin{align}
\begin{pmatrix}
e_{k+1} \\ z_k
\end{pmatrix} = 
\begin{pmatrix}
0 & 0 & 1 \\
1 & .1 & 1 \\
0 & 0 & .1
\end{pmatrix} \begin{pmatrix}
e_k \\ w
\end{pmatrix}. 
\end{align}
With the choice 
\begin{align}
X = \diag(2, .5)
\end{align}
we can compute with MATLAB the solution 
\begin{align}
P = \diag(4.61, -4.61), 
\end{align}
which renders the eigenvalues of the left side term in the relation \eqref{eq:strangeMatrix} to 
\begin{align}
\lambda_1 = -2.35, \; \lambda_2 = -1.21 \text{ and } \lambda_3 = -.488.
\end{align}
Hence, according to the Theorem \ref{thm:stabViaP}, the system is stable for all $\alpha \in [0,2]$. 

\begin{exam}
	\label{ex:Rob:robvsN}
	In the chapter \ref{ch:ILCAlg} we said, that the choice of $\beta$ might impact the robustness properties of the Inverse Model Algorithms.
	Let us illustrate it on the system \eqref{eq:ILC:Sys_ex1_origin}, but with uncertain parameter $a \in [\underline{a}, \; \overline{a}]$ and nominal value $a_0 = 4$: 
	\begin{align}
	A = \begin{pmatrix}
	2 & 1 \\  a & 3
	\end{pmatrix}
	\end{align}
			
	For $N = 23$ and $\beta = .1$ our system still stays robust for $\overline a = - \underline a = 2$. If we set $\beta = .4$, the system is not robust even for $\overline a = - \underline a = .1$. 
	
	However, the ability to find the adequate matrices $X$ and $P$ depends on the time horizon $N$. 
	For $N = 20$, we get a much more robust system, for which the stability can be proven for $\overline a = - \underline a = 2$. 
	

		
\end{exam}


	

	
	
	
	

	

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
% which gets us the matrix $L$. Since the matrix $L$ depends on the model we have, it also depends upon the uncertainty. 
%This uncertainty we assume to be $\Delta$, 
%
%We can pull out the uncertainty with the methods from \cite{RC}, and get 
%
% 
%Let us assume, that some parameter in the model is not exactly known. That means, we have some uncertainty which depends on $\delta \in [-1,
%\,1]$ \cite{SchRC}:
%\begin{align}
%\Delta = \Delta(\delta) = \delta I, \; \delta \in [-1, \, 1].
%\end{align}
%
%With the methods described in \cite{SchRC} let us pull out the uncertainty from $L$. Then the performance and uncertainty channel can be written as 
%\begin{align}
%\left[\begin{array}{c}
%e_{k+1} \\ \hline z_k
%\end{array}\right] &= 
%\left[\begin{array}{c| c}
%\c{A} & \c{B} \\
%\hline
%\c{C} & \c{D}
%\end{array}\right]
%\left[\begin{array}{c}
%e_{k}  \\\hline w_k
%\end{array}\right],\\
%w_k &= \delta z_k.
%\end{align}
%
%and $L$ is given via star product
%\begin{align}
%\label{eq: LstarDelta}
%L(\delta) = \delta I \star \left[\begin{array}{c| c}
%\c{A} & \c{B} \\
%\hline
%\c{C} & \c{D}
%\end{array}\right] = \c{A} + \c{B} \delta (I - \c{D}\delta)^{-1}\c{C}. 
%\end{align}
%
%We are interested in monotonic convergence of the algorithms in spite of the uncertainty $\Delta$. That means, we want to ensure the stability of the system $e_{k+1} = L e_k$. 
%This is possible using the following Theorem: 
%
%
%
%The previous Theorem is true for any structured uncertainty $\Delta$. 
%
%
%
%
%
%
%
%
%
%As we have seen before, it is not always possible to achieve perfect convergence. 
%The next Theorem provide a method to check, whether the chosen controller for \eqref{eq:stability} renders the system stable. We consider some uncertainty $\Delta$ from a compact set $\Delf$. 
%\begin{theo}
%	Suppose that there exists some positive definite matrix $X$ satisfying 
%	\begin{align}
%	\begin{pmatrix}
%	I \\ L(\Delta) 
%	\end{pmatrix}^T
%	\begin{pmatrix}
%	-X & 0 \\ 0 & X
%	\end{pmatrix}
%	\begin{pmatrix}
%	I \\ L(\Delta) 
%	\end{pmatrix} = L(\Delta)X L(\Delta) - X \preceq 0 \text{ for all } \Delta \in \Delf.
%	\end{align}
%	Then the discrete dynamical system 
%	\begin{align}
%	e_{k+1} = L(\Delta) e_k, \; k\geq 0
%	\end{align}
%	is stable for all $\Delta \in \Delf$ and for all $e_0 \in \R^{m ( N+1)}$.
%\end{theo}
%\begin{proof}
%	Since $X \succ 0$, there exist some $\alpha, \beta > 0$, such that 
%	\begin{align}
%	\alpha I \prec X \prec \beta I.
%	\end{align}
%	
%	Set $\eta_k := e_k^T X e_k$. Then 
%	\begin{align}
%	e_{k+1} = e_{k+1}^T X e_{k+1} = e_k^T L(\Delta)^T X L(\Delta) e_k \leq e_k X e_k = \eta_k \text{ for all } k\geq 0.
%	\end{align}
%	Hence
%	\begin{align}
%	\alpha || e_k ||^2 \leq \eta_k \leq \beta ||e_k||^2 \text{ for all } k\geq 0,
%	\end{align}
%	and it follows
%	\begin{align}
%	||e_k||^2 \leq \frac{1}{\alpha}\eta_k \leq \frac{1}{\alpha}\eta(0) \leq \frac{\beta}{\alpha} ||e_0||^2. 
%	\end{align}
%	Taking the square root yields the claim. 
%\end{proof}
%
%
%
%
%
%
%
%
%
% 
% 
% 
% 
% 
% 
%\section{Robust Zero Monotonic Convergence for Parametric Uncertainty}
%Let us assume, that some parameter in the model is not exactly known. That means, we have some uncertainty which depends on one parameter $\delta \in [-1,
%\,1]$ \cite{SchRC}:
%\begin{align}
%\Delta = \Delta(\delta) = \delta I, \; \delta \in [-1, \, 1], 
%\end{align}
%and since $L = L(\delta)$. 
%
%With the methods described in \cite{SchRC} let us pull out the uncertainty from $L$. Then the performance and uncertainty channel can be written as 
%\begin{align}
%\left[\begin{array}{c}
%e_{k+1} \\ \hline z_k
%\end{array}\right] &= 
%\left[\begin{array}{c| c}
%\c{A} & \c{B} \\
%\hline
%\c{C} & \c{D}
%\end{array}\right]
%\left[\begin{array}{c}
%e_{k}  \\\hline w_k
%\end{array}\right],\\
%w_k &= \Delta (\delta) z_k.
%\end{align}
%
%and $L$ is given via star product
%\begin{align}
%\label{eq: LstarDelta}
%L(\delta) = \delta I \star \left[\begin{array}{c| c}
%\c{A} & \c{B} \\
%\hline
%\c{C} & \c{D}
%\end{array}\right] = \c{A} + \c{B} \delta (I - \c{D}\delta)^{-1}\c{C}. 
%\end{align}
%
%Our goal is to ensure 
%\begin{align}
%||L(\delta)|| < 1 \text{ for all } \delta \in [-1,1]. 
%\end{align}
%
%Using the spectral norm we get 
%\begin{align}
%||L(\delta)|| &= \sigma_{\max}(L(\delta)^T L(\delta)) < 1 \text{ for all } \delta \in [-1,\, 1] \\
%\Leftrightarrow 
%L(\delta)^T L(\delta) - I &\prec 0  \Leftrightarrow
%\begin{pmatrix}
%I \\ L(\delta)
%\end{pmatrix}^T 
%\begin{pmatrix}
%-I & \\ & I
%\end{pmatrix}\begin{pmatrix}
%I \\ L(\delta)
%\end{pmatrix} \prec 0 \text{ for all } \delta \in [-1,\,1].
%\end{align}
%
%It can be achieved using following result: 
%
%
%
