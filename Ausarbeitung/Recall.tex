\chapter{Recap} 
Topics to be considered: 
\begin{enumerate}
	\item Vector norms 
	\item GP 
	\item direct sum 
\end{enumerate}
Add Monotonic convergence! 



By theorem \ref{thm: relGandK}, the reight inverse of $G = G(A,B,C,D)$ exists if and only if the matrix $D$ has full row rank. 

The choice of $\beta$ is the only decision to make in this algorithm. For $\beta$ close to 0 or 2, we get slower convergence and better robustness. For $\beta = 1$ we get convergence in one iteration, but as discussed above, this choice might be not robust at all. {\color{red} evtl kommen Robustheit Beispiele}. 

If there exists (only) a left inverse of $G$, we can also calculate a better choice of input $u$. But in this algorithm we do not have guarantee of zero-convergence. 


%\begin{alg}
%	\label{alg: leftInv}
%	Let the matrix $G$ have a left inverse $G_L$: 
%	\begin{align*}
%	G_L G = I. 
%	\end{align*}
%	The Left Inverse Model Algorithm is characterized by choosing $K_0 = \beta G_L$, where $\beta$ is a real scalar ''learning gain''. Feedback interconnection \eqref{eq: errinL} has the form 
%	\begin{align}
%	\label{eq: errLeftInv}
%	e_{k+1} = (I- \beta G G_L) e_k = (I+  \left[(1-\beta)^k - 1\right] G G_L) e_0, \; k\geq 0.
%	\end{align}
%	Monotonic convergence to 
%	\begin{align}
%	\label{eq: leftInvErrLim} 
%	e_\infty  = \lim_{k\to\infty} e_k = (I - G G_L)e_0
%	\end{align} is guaranteed if and only if 
%	\begin{align*}
%	0 <\beta < 2.
%	\end{align*}
%\end{alg} 

