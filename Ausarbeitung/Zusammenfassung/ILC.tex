\documentclass[12pt,ngerman]{article}
\usepackage[utf8]{inputenc}
\usepackage[utf8]{inputenc} 
\usepackage{latexsym} 
\usepackage{amsfonts} 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{MnSymbol}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}
\usepackage{blindtext}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{bbm}
\usepackage{amsthm}
\usepackage[english]{babel}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{xcolor}

% Formatierung
\topmargin -2cm 
\textheight 24cm
\textwidth 16.0 cm 
\oddsidemargin -0.1cm

%Definitionen: 
%D1: Letters
\def\N{\mathbb{N}}
\def\Z{\mathbb{Z}}
\def\R{\mathbb{R}}
\def\C{\mathbb{C}}
\def\Q{\mathbb{Q}}

\def\F{\mathcal{F}}
\def\B{\mathcal{B}}
\def\P{\mathcal{P}}
\def\A{\mathcal{A}}

\def\one{\mathds{1}}
%D2: Klammern
\def\l{\left(}
\def\r{\right)}
%D3: Thm
\newtheorem{theorem}{Theorem}
\newtheorem{alg}{Algorithm}


\definecolor{pink}{rgb}{1.0, 0.0, 0.5}
\begin{document}
	

	\section{Supervectors and Notation}
	
     Let $(A,B,C,D)$ be a $l$-input, $m$-output, state dimension $n$, linear, time-invariant, discrete, state space model of a dynamical system written in the form 
     \begin{align*}
     x(t+1) &= Ax(t) + B u(t), x(0) = x_0, \, t = 0, \dots \, , N-1\\
     y(t) &= Cx(t) + D u(t), \, t  = 0\, \dots \, N.
     \end{align*}
     
     Supervector description: 
     
	\begin{align*}
	y = \begin{pmatrix}
	y(0) \\ y(1) \\ y(2) \\ \vdots \\ y(N)
	\end{pmatrix} \in \mathbb{R}^{m(N+1)},
	u = \begin{pmatrix}
	u(0) \\ u(1) \\ u(2) \\ \vdots \\ u(N)
	\end{pmatrix} \in \mathbb{R}^{l(N+1)}.
	\end{align*}
	
	
	\begin{align*}
	y = Gu + d,
	\end{align*}
	
	where 
	\begin{align*}
	G = G(A, B, C, D) =&
	 \begin{pmatrix}
	D & 0 & \dots & 0 0 0 \\
	CD & D & \dots & 0 0 0\\
	CAB & CB & \dots & 0 0 0\\
	\vdots & \vdots & & \vdots & \vdots\\
	CA^{N-2}B & CA^{N-2}B & \dots & CB & D &0\\
	CA^{N-1}B & CA^{N-2}B & CA^{N-2}B & \dots &CB & D
	\end{pmatrix}\\
	d = d(C, A, x_0) = & \begin{pmatrix}
	Cx_0 \\ CAx_0 \\ CA^2x_0 \\ \vdots \\ CA^N x_0
	\end{pmatrix} \in \mathbb{R}^{m(N+1)}.
	\end{align*}
	
	$r \in \mathbb{R}^{m(N+1)}$ denotes reference signal, 
	\begin{align*}
	e_k = r - y_k, \, k\leq 0
	\end{align*}
	denotes the error on iteration $k$. 
	
	General formula for unit memory algorithm used here: 
	
	\begin{align}
	u_{k+1} &= u_k + K_0 e_k &\text{ \textbf{(The Input Update Rule)}}, \\
	\text{ with } y_k &= Gu_k + d &\text{ \textbf{(Plant Dynamics)}}
	\end{align}
	for some iteration matrix $K_0$ (the corresponding algorithm is named CGA, you can find it \href{run ./MATLAB/CGA.m}{here}).
	
	
	
	\section{Inverse Model Algorithms}
	
	The idea of the inverse algorithms is just to take the inverse of the system $G$ and from the relation 
	\begin{align*}
	y = G u
	\end{align*}
	and get the perfect $u_\infty$ as 
	\begin{align*}
	u_\infty = G^-1 (r- y).
	\end{align*}
	
	\subsection{Right Inverse Model Algorithm} 
	\textbf{Input update low:}
	\begin{align}
	u_{k+1} = u_k + \beta G_R e_k,
	\end{align}
	where $G_R$ denotes the right inverse of $G$ and $\beta$ iss a real scalar ''learning gain''. 
	
	\textbf{Error evolution} results in
	\begin{align*}
	e_{k+1} = (1-\beta) e_k
	\end{align*}
	or, equivalently, 
	\begin{align*}
	e_{k+1} = (1-\beta)^k e_0,
	\end{align*}
	
	Algorithm obviously converges for $0 < 
	\beta < 2$. For $\beta$ close to 0 or 2, we get slow convergence with good robustness properties. For $\beta$ close to unity we have rapidly convergence, but with reduced degree of robustness. For $\beta = 1$ we get convergence in one iteration. 
	Implemented Algorithm you can find \href{run ./MATLAB/RIA.m}{here}. 
	
	\subsection{Left Inverse Model Algorithm}
	
Left inverse model algorithms results just by taking left instead of right inverse matrix. 

	\textbf{Input update low:}
	\begin{align}
	u_{k+1} = u_k + \beta G_L e_k,
	\end{align}
	where $G_L$ denotes the left inverse of $G$ and $\beta$ is a real scalar ''learning gain''. 
	
	\textbf{Error evolution} results in 
	\begin{align*}
	e_{k+1} = (1-\beta) e_k
	\end{align*}
	or, equivalently, 
	\begin{align*}
	e_{k+1} = (1-\beta)^k e_0,
	\end{align*}
	Implemented Algorithm you can find \href{run ./MATLAB/LIA.m}{here}. 
	
	It is worth to say that the left inverse model algorithm doesn't always return a perfect solution: if the initial error doesn't lie in the image of $G$, the error converges to $e_\infty$, which is the projection of $e_0$ onto the kernel of $(I - G G_L)$: 
	\begin{align*}
    \lim_{k \to \infty} e_k = e_\infty = (I - G GL)e0.
	\end{align*}
	
	\subsection{Robustness}
	
	{\color{pink} Wird ergänzt. Die Algorithmen habe ich eher noch nicht getestet.}
	
We consider the left multiplicative uncertainty model $H:= UG$. Then the uncertain error evolution results in
\begin{align}
e_{k+1} = (I - \beta U ) e_k.
\end{align}
\begin{theorem}
	Consider the Right Inverse Model algorithm with $\beta$ in range of $0<\beta<2$. Then, a necessary and sufficient condition for robust monotonic convergence in the presence of the left multiplicative modeling error $U$ is that
	\begin{align*}
	|| (I - \beta U) || \leq 1.
	\end{align*}
\end{theorem}
{\color{pink}There are also other tests, which I'll write down later. The main idea is that if $||I - \gamma U||\leq \delta$ we can choose $\beta$ in some range $\beta \in (0, \, \beta^*)$ for some $\beta^*>0$, and guarantee robust convergence. The lower bound for $\beta^*$ is $\beta^{**} = \gamma \l \frac{1-\delta}{1+\delta}\r$.}
%TODO: Mehr schreiben 
\subsection{Relaxation}
Kommt noch, lässt Robustheit verbessern 
\subsection{Robustness and Non-monotonic Convergence} 
Kommt noch, es geht um die Gewichtung des Systems mit einem epsilon, sodass die Voraussetzungen an die robuste Konvergenz erfüllt sind. 


\section{Gradient Algorithms}
Gradient algorithms are another example of methods, which achieve monotonic convergence. 

\subsection{The Steepest Descent Algorithm for Iterative Control}
\textbf{Input update rule: }
\begin{align}
u_{k+1} = u_k + \beta G^* e_k,
\end{align}
\textbf{error evolution:}
\begin{align}
e_{k+1} = L e_k = L^k e_0, \, L = I - \beta G G^*.
\end{align}

$G^*$ denotes here conjugate transpose of the matrix $G$, $\beta$ is as before a ''learning gain''. 

$\beta$ must be chosen between 0 and $\frac{2}{||G^*||^2}$ to insure the convergence. 
In particular, for $0<\beta<\frac{2}{||G^*||^2} $ and for any initial error $e_0$: 

\begin{enumerate}
	\item \begin{align}
	\lim_{k \to \infty} e_k = e_\infty = P_{ker[G^*]e_0},
	\end{align}\label{eq: eInf}
	 where $P_{ker[G^*]e_0}$ is the self adjoint, positive definite, orthogonal projection operator onto $ker[G^*]$. 
	\item If the initial error $e_0 \in \overline{\text{Image}[GG^*]|}$, then 
	\begin{align}
	\lim_{k \to \infty} = 0.
	\end{align}
\end{enumerate}
Implemented algorithm you can find \href{run ./MATLAB/SDA.m}{here}. 

\subsection{Suppression of Eigenvalues}
In steepest descent algorithms we've chosen gain $\beta$ as constant. It allows much more flexibility, if we set for each iteration different $\beta_k$. Especially if we let $\beta_k$ depend on eigenvalues of $GG^*$. Next theorem give an insight, why it could be a good choice. 
\begin{theorem}
	Let $\lambda_{j1}, \, \lambda_{j2}, \, \lambda_{j3}, \, \dots$ be a chosen ordering of the non-zero eigenvalues of $GG^*$ and suppose that $GG^*$ has $q$ zero eigenvalues. Then the parameter varying iterative control algorithm 
	\begin{align*}
	u_{k+1} = u_k + \beta_{k+1} G^*e_k,  \, \text{ with the choice } 1 - \beta_{k+1}\lambda_{j_{k+1}} = 0,
	\end{align*}
	converges to the limit $e_\infty$ defined by \eqref{eq: eInf} in a finite number of iterations. 
\end{theorem}

The problem with this algorithm is that small values of $\lambda{j}$ will lead to very large values of $\beta_j$. To stay within the range of convergence, we only can use the gains in the range of $0<\beta_k ||G||^2 <2$, which is satisfied, only if $\lambda_{j} < \frac{2}{||G||^2}$. In this case the algorithm can, in principle, be used to eliminate all components with eigenvalues satisfying this inequality. 

{\color{pink} Though this algorithm is illustrated only as theoretical motivation, I've implemented it to see how significant is the difference between the following algorithm and this one. The implementation you can find  \href{run ./MATLAB/SDA_beta_as_evs.m}{here}}.

\begin{alg}
	Choose a finite number $N_p$ of points $p_1, \, p_2, \, \dots$ spread over the half-open interval $(||G||^2, ||G||^2]$ and set the gain $\beta_k = p_k^{-1}$ on any iteration $k\leq N_p$. If, on subsequent iterations, a fixed value in the range of $0<\beta ||G||^2 < 2$ os used, it follows, that the resultant error sequence has the form 
	\begin{align}
	    e_k &= \l \prod_{j=1}^k (I - \beta_j G G^*)\r e_0, \, \text{ for } 1\leq k \leq N_p, \text{ and } \\
	    e_k &= (I - \beta GG^*)^{(k-N_p)}e_{N_p}, \, \text{ for } k \geq N_p.
    \end{align}
\end{alg}

We can consider $e_{N_p}$ here as the initial error for the remainder of the iterative process. For large $N_p$, the approximation is good enough to return much better results as classical steepest descent algorithm. The implementation you can find here.

\section{Model Reducing} 
{\color{red} Hier habe ich ein Problem :( Kap.7.5.1}


In the steepest descent algorithm we've chosen $K_0 = G^*$. This time let's take $K_0 = K^*$ for some matrix $K$, which is simpler than, but not necessarily the same as $G$. 
''Typically, it could be envisaged that, by choosing $K$ as simplified model of $G$ with reduced state dimension, the computational load when computing the input signal $u_{k+1}$ from $u_k$ and $e_k$ will be reduced whilst simultaneously satisfying other conditions (such as those discussed in what follows) that ensure monotonic convergence '' (c)Book

\begin{alg}
	Using the notation defined above, an Iterative Algorithms defined above using the adjoint $K^*$ of a system $(A_K,\, B_K,\, C_K,\, D_K)$, has the 
	\textbf{input update rule:}
	\begin{align}
	u_{k+1} = u_k + \beta K^* e_k.
	\end{align}
	It is \textbf{assumed} that both $G$ and $K$ are asymptotically stable and that $rank(D) = rank(D_K) = \min\{m,l\}$.
\end{alg}

In case $m\leq l$ if
\begin{align}
GK^* +KG^* > \beta KG^*GK^*  \text{ (eq. 7.70 in the book)}
\end{align}
the monotonic convergence is guaranteed. 

In case $m\geq l$ monotonic convergence to 0 is guaranteed if 
\begin{align}
K^* G + G^*K > \beta G^* K K^* G. \text{ (eq. 7.95 in the book)}
\end{align}

{\color{pink} Im Buch betrachtet er $G = KU$ für irgeneine Matrix $U$., schreibt es aber nicht in die Voraussetzungen rein (s. 185). Kann man das eigentlich für balred garantieren? Ich schaue mir das Kapitel nochmals an, wahrscheinlich habe ich da igendwas verapasst.}

\subsection{Robustness }

\subsection{Relaxaion}
Kommt noch 

\subsection{$\varepsilon$-Weighted Norms}
Kommt noch 













	
	
	
	
	
	
	
		
	



\end{document}
