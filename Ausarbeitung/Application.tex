\chapter{Application}
\label{ch:Allpication}

The listed algorithms have their theoretical benefits, but they might not be good for real issues. The numeric obstacles require the trade-offs between robustness, stability, and calculation time. The computer memory has limited capacity; systems discretization adds the plant errors. We considered the examples do not have a physical reference system. Now, we try to apply the algorithms on the more ''real'' plant and see the problems that occur. 


\section{The case $D = 0$}
		\begin{exam}
		 We consider a dynamical system, which describes a flexible satellite. As known from \cite{SchRC} the continuous-time transfer function has the form  
		\begin{align}
		G(s) = \frac{.036(s + 25.28)}{s^2(s^2 + .0396s + 1)}. 
		\end{align}
		As input we choose the torque on first mass, and as output is set the position $\theta_2$. 
		
		Since we are working with discrete time systems, we discretize it for sample time $T_s = 10^{-3}$  and get the state space description 
		\begin{align}
		\label{eq:Appl:satellite_sys}
		\left( \begin{array}{c | c}
		A & B \\ \hline C & D
		\end{array}\right) = 
		\left(\begin{array}{c c c c | c}
	    1.000  &  0.005  &  0.000  &  0.000 & 1.047\cdot 10^{-13}\\
		0  &  1.000  &  0.001  &  0.000 & 8.333\cdot 10^{-11}\\
		0  &       0  &  1.000  &  0.001 & 2.5\cdot 10^{-7}\\
		0  &       0  & -0.001  &  1.000 & 5\cdot 10^{-4}\\ \hline
        0.362 & 0.072&  0 & 0 & 0
		\end{array}\right). 
	\end{align}
	\end{exam}

In this example, the matrix $D$ is zero, what makes our algorithms not applicable. 
Moreover, the  $D$-part's neglecting is typical: the system output must mostly image the system state and not the known input we send. 

To see more precisely, the input update rule has the form 
\begin{align}
y(t) = C x(t) = C(A x(t) + B u(t)) \text{ for } t = 0, 1,2, \dots, N. 
\end{align}

With other words, the input $u(t)$ begins to impact the output by first iteration, if the matrix $CB$ is not a zero-matrix. That means, that the \textit{relative degree} of the system equals 1. We define it mathematically precise: 
\begin{defi}
	The relative degree of the system $(A, B, C, D)$ is 0 if $D \neq 0$. For $D = 0$ it is the smallest integer $k$ for which $	C A^{k-1} B \neq 0$.
\end{defi} 
Let us denote it with $k^*$. Then the new system we consider can be written as 
\begin{align}
x(t+1) &= A x(t) + B u(t), \\
y(t) &= C x(t) + C A^{k^*} B u(t),  \text{ for } t = k^*, k^* + 1, \dots N. 
\end{align}

We need to put the first iterations away and define a new initial condition $x(k^*)$. The start input condition is then $u(k^*)$. 

For new system all the requirements we considered are satisfied. However, we do not get a better solution for $t = 0, 1, \dots, k^\star - 1$. 


\begin{example}[continued]
	
\textit{For our system the relative degree is 1, as }
	\begin{align}
	CB = 6.038\cdot 10^{-12} \neq 0. 
	\end{align}
	
\textit{We calculate the solution with LQR. Result is illustrated in Figure \ref{img:Appl:Sat_LQR}.}
	
	\begin{figure}[ht]
		\centering
		\includegraphics[width=\textwidth]{fig/Ex3_LQR.jpg}
		\caption{LQR Solution for the system \eqref{eq:Appl:satellite_sys}}
		\label{img:Appl:Sat_LQR}
	\end{figure}
	
\textit{It looks like something we can improve. However, for $Ts = 10^{-3}$s we get $N = 10^3$ time steps even for one iteration second. The matrix $G$ has  $10^9$ elements and can not easily be handled.}
\end{example}

\section{Long time horizon}

	In example \ref{ex:ILC:badIA} the Inverse Model Algorithm is not applicable even for $N = 50$. 
	It means, we need a trade off between the time horizon and robustness.
	However, for real problems the sample times are mostly small, and hence even for one second of the simulation we get huge number for time horizon. Already for SISO system the matrix $G$ will have $N^2$ elements. Our algorithms become unusable for casual applications since they need vast masses of memory.	
	
	One possibility to reduce the wasted memory is to split our system in the smaller ones. Let us denote the time horizon length with $N_{\max}$, and choose an $N$ with $1<N\leq N_{\max}$. 
	We can apply our algorithms on the systems with the same state space description, but different initial values. For simplicity, let us assume, that we can divide our time horizon $0,1,2,\dots, N_{\max}$ in $p$ equal parts with time horizon of length $N$, and the matrix $D = 0$. 
	
	Then the initial condition for the $p$-th application of the algorithm changes as 
	\begin{align}
	x_{0_p}= A^p x_0 + \sum_{\rho = 0}^{p - 1} \sum_{t = \rho (N + 1)}^{\rho (N+2)} A^{(N+2)\rho - t} B u(t). 
	\end{align}
	
	However, for large values of $p$ the term $A^{(N+2)\rho - 1 }\approx 0$ for $\rho > p$.
	We can estimate it as 
	\begin{align}
	||x_{0_p}|| &\leq  \sum_{\rho = 0}^{p^{\star}} \sum_{t = \rho (N + 1)}^{\rho (N+2)} ||A^{(N+2)\rho - t} B u(t)|| \\
	&+
	\underbrace{\sum_{\rho = p^\star+1}^{p^{\star}} \sum_{t = \rho (N + 1)}^{\rho (N+2)} ||A^{(N+2)\rho - t} B u(t)||}_{\approx 0} + \underbrace{||A^p x_0||}_{\approx 0} 
	\end{align}
	for some $1<p^{\star}<p$ large enough, and then we need to calculate only the first $p^\star (N+2)$ terms, independent on groving $p$. 
	
	But what if $N < p^\star$? Our matrix $G$ contains then these small terms, which cause the bad condition number. 
	We can remove them from $G$ as well -- this also has an advantage, that we can use the sparse matrices for calculations and decrease the used memory drastically. 
	
	To be able to chose this number $p^\star$, let us consider the ''reduced'' lifted system  
	\begin{align}
	\t y = \t G + d 
	\end{align}
	with 
	\begin{align}
	\t G = \begin{pmatrix}
	D  \\
	CB & D \\
	C A B & CB & D\\
	\vdots & \vdots & \vdots & \ddots \\
	C A^{p^\star-1} B & C A^{p^\star-2}B & C A^{p^\star-3}B &\dots& D \\
	0           & C A^{p^\star-1} B & C A^{p^\star-2} B & \dots & CB & D\\
	0 & 0 & C A^{p^\star-1} B & \dots & CAB & CB & D \\
	\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots \\
	0 & 0 & 0 & \dots & C A^{p^\star-1}& C A^{p^\star-2}B& CA^{p^\star-3}B &\dots & D
	\end{pmatrix}.
	\end{align}
	
	Then for a given tolerance $\varepsilon>0$ and for time $t \in \{1,2, \dots , N_{\max}\}$, the difference between the ''normal'' and ''reduced'' output equals
	\begin{align}
	||y(t) - \t y(t)|| = ||\sum_{\tau = p^\star}^{t} C A^{\tau} B u(p^\star - \tau)|| \leq ||C|| \, ||B|| \sum_{\tau = p^\star}^t ||A||^\tau \max_{\eta = 0}^{p^\star}||u(\eta)||. 
	\end{align}
	
	The input estimation $\max_{\eta = 0}^{p^\star}||u(\eta)||$ can be often replaced by some $||u_{\max}||$, where $u_{\max}$ is the largest possible input. 
	
	Then for a given tolerance $\varepsilon$ we get 	
	\begin{align}
	\varepsilon \geq ||C||\, ||B|| \frac{||A||^{p^\star} - ||A||^{N_{\max}}}{1 - ||A||} ||u_{\max}|| \geq ||C|| \, ||B|| \frac{||A||^k - ||A||^{N_{\max} + \t N}}{1 - ||A||}||u_{\max}||.
	\end{align}
	for any $\t N \geq 0$. 
	
	We can see that for a groving time horizon our $p^\star$ stays the same, without need to determine it again. 
	
	\section{Online learning} 
	TODO: ersetzen durch's Beispiel und sagen das als notiz
	
	In the real practice we usually do not have all the data immediately. We get the system output gradually over time, and hence we can apply only the input and output signals so-fare. That means we learn our system ''online'', and feed the current data we have already got. 
	
 	As we have seen before, there is no need to calculate the reduced system for a groving time horizon length. 
	
	
	
	
	
	